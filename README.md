# ğŸŒ Atlas

[![Version](https://img.shields.io/badge/version-v1.1.0-blue.svg)](https://github.com/juliuspleunes4/Atlas/releases/tag/v1.1.0)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://img.shields.io/badge/tests-324%20passing-brightgreen.svg)](tests/)

**A from-scratch language model implementation with GGUF export.**

Atlas is a complete pipeline for building, training, and deploying decoder-only transformer language models. The project focuses on clarity, modularity, and the ability to export trained models to GGUF format for efficient inference.

## âœ¨ Features

- **ğŸ¯ Clean Implementation**: Decoder-only transformer architecture built from scratch with PyTorch
- **ğŸ”„ Complete Pipeline**: Training, evaluation, inference, and export all in one place
- **ğŸ’¾ Memory Efficient**: 8-bit optimizer support (75% memory reduction) for large models on consumer GPUs
- **âš¡ Reliable Checkpointing**: Built-in mid-epoch checkpoint saving every N steps
- **ğŸ“¦ GGUF Export**: Convert trained models to GGUF format for use with llama.cpp
- **ğŸ§© Modular Design**: Well-organized codebase with clear separation of concerns
- **âœ… Comprehensive Testing**: 324 tests covering all major components

## ğŸ“Š Project Status

ğŸš§ **Under active development** ğŸš§

Atlas is currently in early development. See [`docs/ROADMAP.md`](docs/ROADMAP.md) for the full development plan.

**Completed:**
- âœ… Phase 0: Project Foundation
- âœ… Phase 1: Configuration System (32 tests)
- âœ… Phase 2: Tokenizer Integration (27 tests)
- âœ… Phase 3: Model Architecture (51 tests - **+9 gradient checkpointing tests**)
- âœ… Phase 4: Data Pipeline (72 tests)
- âœ… Phase 5: Training Loop (62 tests - **+6 auto-resume tests**)
- âœ… Phase 5.5: Training Script (13 tests)
- âœ… Phase 6: Inference & Generation (21 tests)
- âœ… Phase 6.3: Inference Script (12 tests)
- âœ… Phase 7: GGUF Export (17 tests)

**Skipped (for now):**
- â­ï¸ Phase 8: End-to-End Integration (individual components thoroughly tested)

**Upcoming:**
- Phase 9-10: Advanced features and optimization

**Total: 324 passing tests** âœ¨

## âš™ï¸ Model Configurations

Atlas provides multiple pre-configured model sizes optimized for different hardware capabilities:

| Config | Parameters | Layers | Hidden Size | Heads | Sequence Length | Batch Size | VRAM Usage | Best For |
|--------------|-----------|--------|-------------|-------|----------------|------------|------------|----------|
| **TINY** | ~40M | 8 | 512 | 8 | 512 | 4 (Ã—4 accum) | 4-6 GB | Testing, debugging, low-end GPUs |
| **SMALL** | ~124M | 12 | 768 | 12 | 1024 | 24 (Ã—2 accum) | 6-8 GB | Quick experiments, prototyping |
| **DEFAULT** | ~350M | 24 | 1024 | 16 | 1024 | 16 (Ã—2 accum) | 12-14 GB | **Recommended** for most users |
| **LARGE** | ~500M | 30 | 1280 | 20 | 1024 | 8 (Ã—4 accum) | 14-15 GB | Maximum quality, high-end GPUs |
| **XLARGE** | ~500M | 30 | 1280 | 20 | 1024 | 2 (Ã—16 accum) | 8-10 GB | Max params with memory safety |
| **ULTRA** | ~650M | 30 | 1280 | 20 | 256 | 1 (Ã—16 accum) | 3-5 GB | **Cool & Quiet** - Low GPU temp |

**Configuration Details:**

- **TINY** (`configs/tiny.yaml`): Ultra-lightweight for testing and development
  - Effective batch size: 16 (4 Ã— 4 gradient accumulation)
  - Training time: ~1-2 days on RTX 3080 for 10K steps
  - Good for verifying pipeline, debugging, or running on older GPUs
  
- **SMALL** (`configs/small.yaml`): GPT-2 Small equivalent
  - Effective batch size: 48 (24 Ã— 2 gradient accumulation)
  - Training time: ~3-5 days on RTX 3080 for 20K steps
  - Capable of basic text generation and learning patterns
  
- **DEFAULT** (`configs/default.yaml`): GPT-2 Medium equivalent (Recommended)
  - Effective batch size: 32 (16 Ã— 2 gradient accumulation)
  - Training time: ~1-2 weeks on RTX 3080 for 50K steps
  - Optimized for RTX 5060 Ti 16GB with safe memory margin
  - Best balance of quality and training time
  
- **LARGE** (`configs/large.yaml`): Maximum quality
  - Effective batch size: 32 (8 Ã— 4 gradient accumulation)
  - Training time: ~2-3 weeks on RTX 3080 for 80K steps
  - Close to 16GB VRAM limit - ensure good cooling
- **XLARGE** (`configs/xlarge.yaml`): Memory-optimized maximum size
  - Effective batch size: 32 (2 Ã— 16 gradient accumulation)
  - Same 500M parameters as LARGE but uses 40% less VRAM
  - Training time: Similar to LARGE (~2-3 weeks for 80K steps)
  - **Best choice for maximizing model size while staying within GPU limits**

- **ULTRA** (`configs/ultra.yaml`): Extreme low-temperature optimization
  - Effective batch size: 64 (1 Ã— 64 gradient accumulation)
  - Same 500M parameters, shorter sequences (256 tokens)
  - Uses absolute minimum VRAM (batch_size=1) + gradient checkpointing
  - Runs COOLEST of all configs - minimal GPU load/temperature
  - Training time: Slower than XLARGE due to extreme accumulation
  - **Best for: Maximum parameters while keeping GPU cool and quiet**

**Choosing a Configuration:**

Use the automated training script to select interactively:
```powershell
.\scripts\run_pipeline.ps1  # Windows
./scripts/run_pipeline.sh   # Linux/Mac
```

Or specify directly:
```bash
python scripts/train.py --config configs/tiny.yaml --train-data data/processed/wikipedia
```

## ğŸš€ Installation

### ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Git

### ğŸ”§ Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/juliuspleunes4/Atlas.git
cd Atlas
```

2. **Create a virtual environment**

**On Windows:**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**On macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **Install Atlas as a package (development mode)**

```bash
pip install -e .
```

5. **Verify installation**

```bash
pytest tests/ -v
```

## ğŸ¯ Quick Start (Recommended)

**The easiest way to get started - fully automated and interactive!**

### Step 1: Clone the repository
```bash
git clone https://github.com/juliuspleunes4/Atlas.git
cd Atlas
```

### Step 2: Download training data
Get the Wikipedia SimpleEnglish dataset from [Kaggle](https://www.kaggle.com/datasets/ffatty/plaintext-wikipedia-simpleenglish) (171 MB zip file)

### Step 3: Place the zip file
```
Atlas/data/raw/archive.zip
```

### Step 4: Run the interactive training pipeline
```powershell
# Windows
.\scripts\run_pipeline.ps1

# Linux/Mac
chmod +x scripts/run_pipeline.sh
./scripts/run_pipeline.sh
```

**That's it!** ğŸ‰ The script will:
- âœ… Check Python and create virtual environment
- âœ… Install all dependencies automatically
- âœ… Prepare the dataset (249K articles)
- âœ… Show you an interactive menu to choose model size
- âœ… Start training with your selected configuration

The script handles everything else automatically and guides you through each step!

### ğŸ”„ Checkpoint Auto-Resume

Atlas automatically detects existing checkpoints and asks if you want to resume training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Checkpoint found: checkpoints/atlas_step_500.pt     â”‚
â”‚  Step: 500                                           â”‚
â”‚  Epoch: 1                                            â”‚
â”‚  Loss: 3.456                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Resume from checkpoint? (y/n):
```

- **Choose "y"**: Continue training from the checkpoint (preserves optimizer state, learning rate, etc.)
- **Choose "n"**: Start a fresh training session (existing checkpoints remain untouched)

This works in:
- Interactive pipeline scripts (`run_pipeline.ps1`, `run_pipeline.sh`)
- Direct training script (`python scripts/train.py`)

To bypass the prompt and force resumption:
```bash
python scripts/train.py --config configs/default.yaml --resume checkpoints/atlas_step_500.pt
```

---

## ğŸ”§ Advanced Usage

### ğŸ“¦ Manual Data Preparation

If you need more control over data preparation:

```bash
# Basic usage
python scripts/prepare_data.py --input data/raw/archive.zip

# Custom output directory
python scripts/prepare_data.py --input data/raw/archive.zip --output data/processed/my_wiki

# List prepared datasets
python scripts/prepare_data.py --list
```

This extracts and organizes 249K articles (~400MB) into the processed directory.

### ğŸ‹ï¸ Manual Training

```bash
# Basic training with a configuration
python scripts/train.py --config configs/default.yaml --train-data data/processed/wikipedia

# Train with validation data
python scripts/train.py \
  --config configs/default.yaml \
  --train-data data/processed/wikipedia \
  --val-data data/processed/validation

# Resume training from a checkpoint
python scripts/train.py \
  --config configs/default.yaml \
  --train-data data/processed/wikipedia \
  --resume checkpoints/atlas_step_5000.pt

# Override config parameters from CLI
python scripts/train.py \
  --config configs/default.yaml \
  --train-data data/processed/wikipedia \
  --max-steps 100000 \
  --save-interval 500 \
  --eval-interval 1000
```

**Available training arguments:**
- `--config`: Path to YAML configuration file (required)
- `--train-data`: Path to training data directory (required)
- `--val-data`: Path to validation data (optional)
- `--output-dir`: Checkpoint directory (default: `./checkpoints`)
- `--resume`: Resume from checkpoint path
- `--max-steps`: Override max training steps
- `--save-interval`: Save checkpoint every N steps (default: 1000)
- `--eval-interval`: Evaluate every N steps (default: 1000)
- `--device`: Device to use (default: auto-detect CUDA)

### ğŸ’¬ Inference

Generate text from a trained model:

```bash
# Single prompt
python scripts/infer.py \
  --checkpoint checkpoints/my_model/checkpoint_step_10000.pt \
  --prompt "Once upon a time" \
  --max-new-tokens 100 \
  --temperature 0.8 \
  --top-k 40 \
  --do-sample
```

Interactive mode:

```bash
python scripts/infer.py \
  --checkpoint checkpoints/my_model/best_model.pt \
  --interactive
```

Batch generation from file:

```bash
python scripts/infer.py \
  --checkpoint checkpoints/my_model/best_model.pt \
  --prompts-file prompts.txt \
  --output-file generated.txt \
  --temperature 0.9 \
  --top-p 0.95 \
  --do-sample
```

### ğŸ“¦ Export to GGUF

Export trained model to GGUF format:

```bash
# Export with float32 (default)
python scripts/export_gguf.py \
  --checkpoint checkpoints/my_model/best_model.pt \
  --output models/atlas_model.gguf \
  --quantization f32
```

Export with float16 for smaller file size:

```bash
python scripts/export_gguf.py \
  --checkpoint checkpoints/my_model/best_model.pt \
  --output models/atlas_model_f16.gguf \
  --quantization f16 \
  --tokenizer gpt2
```

## ğŸ“ Project Structure

```
Atlas/
â”œâ”€â”€ atlas/              # Main package
â”‚   â”œâ”€â”€ model/         # Model architecture (embeddings, attention, transformer blocks)
â”‚   â”œâ”€â”€ tokenizer/     # Tokenization and vocabulary
â”‚   â”œâ”€â”€ training/      # Training loop and optimization
â”‚   â”œâ”€â”€ data/          # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ config/        # Configuration management
â”‚   â”œâ”€â”€ inference/     # Text generation and sampling
â”‚   â”œâ”€â”€ utils/         # Logging, checkpointing, metrics
â”‚   â””â”€â”€ export/        # Model export (GGUF, etc.)
â”œâ”€â”€ tests/             # Test suite
â”œâ”€â”€ scripts/           # CLI scripts
â”œâ”€â”€ docs/              # Documentation
â”‚   â”œâ”€â”€ ROADMAP.md    # Development roadmap
â”‚   â””â”€â”€ CHANGELOG.md  # Change log
â””â”€â”€ README.md          # This file
```

## ğŸ—ï¸ Architecture

Atlas implements a **decoder-only transformer** architecture (GPT-style):

- **Multi-head self-attention** with causal masking
- **Feed-forward networks (MLP)** with configurable activation (GELU/SiLU)
- **Layer normalization** (pre-norm architecture)
- **Learned positional embeddings**
- **Weight tying** between input embeddings and output projection

## ğŸ› ï¸ Development

### ğŸ§ª Running Tests

Run all tests:
```bash
pytest tests/ -v
```

Run specific test file:
```bash
pytest tests/test_config.py -v
```

Run with coverage:
```bash
pytest tests/ --cov=atlas --cov-report=html
```

### ğŸ’ Code Quality

Format code:
```bash
black atlas/ tests/ scripts/
```

Lint code:
```bash
flake8 atlas/ tests/ scripts/
```

Type checking:
```bash
mypy atlas/
```

Run all quality checks:
```bash
black atlas/ tests/ scripts/ && flake8 atlas/ tests/ scripts/ && mypy atlas/ && pytest tests/ -v
```

## ğŸ—ºï¸ Roadmap

See [`docs/ROADMAP.md`](docs/ROADMAP.md) for the complete development plan, which includes:

- Phase 0-1: Project foundation and configuration system
- Phase 2: Tokenizer integration
- Phase 3: Model architecture implementation
- Phase 4: Data pipeline
- Phase 5: Training loop
- Phase 6: Inference and generation
- Phase 7: GGUF export
- Phase 8: End-to-end integration
- Phase 9: Optimization and refinement
- Phase 10: Documentation and release

## ğŸ¤ Contributing

We welcome contributions to Atlas! Whether you're fixing bugs, adding features, improving documentation, or writing tests, your help is appreciated.

### ğŸ¬ Getting Started

1. **Fork the repository** on GitHub
2. **Clone your fork** locally:
   ```bash
   git clone https://github.com/juliuspleunes4/Atlas.git
   cd Atlas
   ```
3. **Create a virtual environment** and install dependencies (see Installation section)
4. **Create a new branch** for your changes:
   ```bash
   git checkout -b feature/your-feature-name
   ```

### ğŸ“ Development Guidelines

#### ğŸ’ Code Quality

- **Format your code** with black:
  ```bash
  black atlas/ tests/ scripts/
  ```
- **Lint your code** with flake8:
  ```bash
  flake8 atlas/ tests/ scripts/
  ```
- **Type check** with mypy:
  ```bash
  mypy atlas/
  ```

#### ğŸ§ª Testing

- **Write tests** for all new features and bug fixes
- **Run the test suite** to ensure nothing breaks:
  ```bash
  pytest tests/ -v
  ```
- **Check test coverage**:
  ```bash
  pytest tests/ --cov=atlas --cov-report=html
  ```
- **Aim for >80% coverage** on new code

#### ğŸ“ Commit Guidelines

- Write clear, descriptive commit messages
- Use conventional commit format:
  - `feat:` for new features
  - `fix:` for bug fixes
  - `docs:` for documentation changes
  - `test:` for test additions/changes
  - `refactor:` for code refactoring
  - `chore:` for maintenance tasks

#### ğŸ“š Documentation

- Update `docs/CHANGELOG.md` with your changes
- Add docstrings to all public functions and classes
- Update README.md if adding user-facing features

### ğŸš€ Submitting Changes

1. **Ensure all tests pass** and code is formatted
2. **Commit your changes**:
   ```bash
   git add .
   git commit -m "feat: add amazing new feature"
   ```
3. **Push to your fork**:
   ```bash
   git push origin feature/your-feature-name
   ```
4. **Create a Pull Request** on GitHub
5. **Address review feedback** if requested

### ğŸ’¡ What to Contribute

Check out the [roadmap](docs/ROADMAP.md) for areas that need work:

- ğŸ”§ **Core Features**: Model architecture, training loop, inference
- ğŸ“ **Documentation**: Tutorials, examples, API docs
- ğŸ§ª **Tests**: Increase coverage, add edge cases
- ğŸ› **Bug Fixes**: Check issues for known bugs
- âœ¨ **Optimizations**: Performance improvements, memory efficiency

### ğŸŒŸ Code of Conduct

- Be respectful and inclusive
- Provide constructive feedback
- Focus on the code, not the person
- Help others learn and grow

## ğŸ“„ License

See [LICENSE](LICENSE) for details.

## â“ FAQ

### How do I activate the virtual environment?

**Windows (PowerShell):**
```powershell
.\venv\Scripts\Activate.ps1
```

**Windows (Command Prompt):**
```cmd
venv\Scripts\activate.bat
```

**macOS/Linux:**
```bash
source venv/bin/activate
```

### How do I deactivate the virtual environment?

Simply run:
```bash
deactivate
```

### Tests are failing, what should I do?

1. Ensure you've activated the virtual environment
2. Make sure all dependencies are installed: `pip install -r requirements.txt`
3. Try running a specific test file to isolate the issue
4. Check if you're using Python 3.8+: `python --version`

### Where should I start contributing?

1. Check the [roadmap](docs/ROADMAP.md) for tasks marked as `[ ]` (not started)
2. Look for `TODO` comments in the codebase
3. Improve test coverage in existing modules
4. Add documentation and examples

## ğŸ™ Acknowledgments

- Inspired by modern transformer architectures (GPT, LLaMA)
- GGUF format from [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
- Built with [PyTorch](https://pytorch.org/)
