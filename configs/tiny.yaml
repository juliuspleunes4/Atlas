# Atlas Training Configuration - TINY MODEL
# Very lightweight for testing and low-end GPUs (~4-6GB VRAM)

# Model architecture (GPT-2-like Tiny - ~40M parameters)
model:
  vocab_size: 50261    # GPT-2 vocab size
  max_seq_len: 512     # Shorter sequences = less memory
  hidden_size: 512     # Smaller hidden dimension
  num_layers: 8        # Fewer layers
  num_heads: 8         # Fewer attention heads
  mlp_ratio: 4.0       # Standard MLP expansion
  dropout: 0.1         # Dropout probability

# Tokenizer settings
tokenizer:
  name: "gpt2"
  encoding: "gpt2"

# Data settings
data:
  max_seq_len: 512     # Match model.max_seq_len
  num_workers: 1       # Reduced workers to save memory

# Training hyperparameters (optimized for low memory)
training:
  learning_rate: 6.0e-4      # Higher LR for smaller model
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Very small batches for low memory
  batch_size: 4              # Small batch size
  gradient_accumulation_steps: 4  # Effective batch = 16
  
  max_steps: 10000           # Quick training
  warmup_steps: 500
  scheduler_type: "cosine"
  
  keep_checkpoints: 2

# Use this for:
# - Testing on low-end GPUs (4-6GB VRAM)
# - Quick experiments and debugging
# - CPU training (will be slow but works)
# - Verifying training pipeline works
# Model size: ~40M parameters
# Memory usage: ~4-6GB VRAM during training
# Effective batch size: 16 (4 batch * 4 accumulation)
# Training speed: Fast (good for iteration)

