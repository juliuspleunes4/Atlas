# Atlas Training Configuration - ULTRA MEMORY-OPTIMIZED LARGE MODEL
# Maximum parameters with absolute minimal memory footprint
# batch_size=1 with extreme gradient accumulation

# Model architecture (~500M parameters)
model:
  vocab_size: 50261    # GPT-2 vocab size
  max_seq_len: 256     # REDUCED from 512 to minimize GPU load/temperature
  hidden_size: 1280    # Keep large hidden size
  num_layers: 30       # Keep deep network
  num_heads: 20        # Keep many heads
  mlp_ratio: 4.0       # MLP expansion ratio
  dropout: 0.1         # Dropout probability

# Tokenizer settings
tokenizer:
  name: "gpt2"
  encoding: "gpt2"

# Data settings
data:
  max_seq_len: 256     # Match model - shorter sequences = less GPU work
  num_workers: 0       # Single worker to reduce RAM usage

# Training hyperparameters (EXTREME memory + LOW GPU LOAD optimization)
training:
  learning_rate: 2.0e-4      # Lower LR for larger model
  weight_decay: 0.01
  max_grad_norm: 1.0
  optimizer_type: "adamw8bit"  # 8-bit optimizer: 75% less memory (requires: pip install bitsandbytes)
  
  # ABSOLUTE MINIMUM batches with extreme gradient accumulation
  batch_size: 1              # Single sample per step (minimum possible)
  gradient_accumulation_steps: 16  # Further reduced to prevent system freeze (was 32, originally 64)
  
  max_steps: 80000           # More steps for larger model
  warmup_steps: 3000
  scheduler_type: "cosine"
  
  gradient_checkpointing: true  # Trade compute for memory (lowers GPU load)
  keep_checkpoints: 2        # Number of step-based checkpoints to keep (epoch checkpoints kept separately)

# EXTREME MEMORY + LOW GPU LOAD OPTIMIZATION:
# - 656M parameter model (30 layers Ã— 1280 hidden)
# - 8-bit optimizer: Stores momentum in 8-bit instead of 32-bit (75% memory reduction)
#   * Standard AdamW: ~1.9GB optimizer states for 655M params
#   * 8-bit AdamW: ~0.5GB optimizer states (saves ~1.4GB!)
# - Batch size = 1 (absolute minimum, ~85% less VRAM than batch=8)
# - Sequence length = 256 (75% less memory & compute than 1024, 50% less than 512)
# - Gradient accumulation = 16 (prevents system freeze from optimizer memory spike)
# - Gradient checkpointing enabled (trades recomputation for lower memory/temp)
# - Single data worker (reduces RAM pressure)
# - Should use ~3-5GB VRAM with COOLEST running temperature
# - Training will be slower but runs MUCH COOLER than XLARGE
# - Requires: pip install bitsandbytes (for 8-bit optimizer)
# - Best for: Maximizing parameters while keeping GPU cool and quiet

# WHEN TO USE:
# - Your GPU keeps running out of memory with XLARGE
# - You want maximum model size but have memory constraints
# - You're willing to trade training speed for model capacity
