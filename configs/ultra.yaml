# Atlas Training Configuration - ULTRA MEMORY-OPTIMIZED LARGE MODEL
# Maximum parameters with absolute minimal memory footprint
# batch_size=1 with extreme gradient accumulation

# Model architecture (~500M parameters)
model:
  vocab_size: 50261    # GPT-2 vocab size
  max_seq_len: 512     # Reduced from 1024 to save memory
  hidden_size: 1280    # Keep large hidden size
  num_layers: 30       # Keep deep network
  num_heads: 20        # Keep many heads
  mlp_ratio: 4.0       # MLP expansion ratio
  dropout: 0.1         # Dropout probability

# Tokenizer settings
tokenizer:
  name: "gpt2"
  encoding: "gpt2"

# Data settings
data:
  max_seq_len: 512     # Match model
  num_workers: 0       # Single worker to reduce RAM usage

# Training hyperparameters (EXTREME memory optimization)
training:
  learning_rate: 2.0e-4      # Lower LR for larger model
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # ABSOLUTE MINIMUM batches with extreme gradient accumulation
  batch_size: 1              # Single sample per step (minimum possible)
  gradient_accumulation_steps: 32  # Effective batch = 32 (same as large)
  
  max_steps: 80000           # More steps for larger model
  warmup_steps: 3000
  scheduler_type: "cosine"
  
  keep_checkpoints: 2        # Number of step-based checkpoints to keep (epoch checkpoints kept separately)

# EXTREME MEMORY OPTIMIZATION:
# - 656M parameter model (30 layers Ã— 1280 hidden)
# - Batch size = 1 (absolute minimum, ~85% less VRAM than batch=8)
# - Sequence length = 512 (50% less memory than 1024)
# - Gradient accumulation = 32 (maintains effective batch = 32)
# - Single data worker (reduces RAM pressure)
# - Should use ~5-7GB VRAM
# - Training will be slower but WILL FIT on constrained GPUs
# - Best for: Maximizing parameters when memory is critical

# WHEN TO USE:
# - Your GPU keeps running out of memory with XLARGE
# - You want maximum model size but have memory constraints
# - You're willing to trade training speed for model capacity
