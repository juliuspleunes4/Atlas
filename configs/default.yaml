# Atlas Training Configuration
# Optimized for RTX 5060 Ti 16GB VRAM

# Model architecture (GPT-2 Medium sized - ~350M parameters)
model:
  vocab_size: 50261    # GPT-2 vocab size (required for tiktoken)
  max_seq_len: 1024    # Maximum sequence length
  hidden_size: 1024    # Hidden dimension (increased from 768)
  num_layers: 24       # Number of transformer layers (increased from 12)
  num_heads: 16        # Number of attention heads (increased from 12)
  mlp_ratio: 4.0       # MLP expansion ratio
  dropout: 0.1         # Dropout probability

# Tokenizer settings
tokenizer:
  name: "gpt2"         # Tokenizer name (uses tiktoken)
  encoding: "gpt2"     # Encoding name (gpt2, cl100k_base, etc.)

# Data settings
data:
  max_seq_len: 1024    # Must match model.max_seq_len
  num_workers: 2       # Number of dataloader workers (2 for better throughput)

# Training hyperparameters (optimized for 16GB VRAM)
training:
  # Optimization
  learning_rate: 3.0e-4      # Peak learning rate (slightly lower for larger model)
  weight_decay: 0.01         # Weight decay coefficient
  max_grad_norm: 1.0         # Gradient clipping norm
  
  # Batch sizes (optimized for 16GB VRAM)
  batch_size: 16             # Batch size per device (increased from 8)
  gradient_accumulation_steps: 2  # Effective batch = 16 * 2 = 32
  
  # Schedule
  max_steps: 50000           # Total training steps (~1.6B tokens with batch 32)
  warmup_steps: 2000         # Warmup steps (4% of max_steps)
  scheduler_type: "cosine"   # LR scheduler (cosine, linear, constant)
  
  # Checkpointing
  keep_checkpoints: 3        # Number of step-based checkpoints to keep (epoch checkpoints kept separately) (saves disk space)

# Notes:
# - Model size: ~350M parameters (similar to GPT-2 Medium)
# - Memory usage: ~12-14GB VRAM during training (safe for 16GB)
# - Effective batch size: 32 (16 batch * 2 gradient accumulation)
# - Training on 170MB Wikipedia: ~30K-50K steps recommended
# - Total tokens per epoch: ~31M tokens
# - With batch 32 & seq 1024: ~32K tokens per step
# - Full dataset coverage: ~1000 steps per "epoch"
